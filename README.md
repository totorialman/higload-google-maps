# Google Maps 

## 1. Тема и целевая аудитория
**Google Maps** - сервис Google для просмотра карт, спутниковых снимков, панорам улиц и построения маршрутов в реальном времени.

### Функционал MVP
- Просмотр карты  
- Построение маршрутов  
- Поиск мест и организаций  
- Просмотр информации о местах  
- Загрузка офлайн-карт  

### Целевая аудитория
MAU: **2 млрд пользователей**[^1]

| Страна   | Доля[^2] | Пользователей в месяц |
|----------|------|------------------------|
| США      | 11,32 % | 226 млн |
| Турция   | 8,77 %  | 175 млн |
| Япония   | 8,24 %  | 165 млн |
| Индия    | 6,64 %  | 133 млн |
| Бразилия | 6,14 %  | 123 млн |
| Остальные| 58,89 % | 1178 млн |

---

## 2. Расчет нагрузки
### Продуктовые метрики

| Метрика | Значение |
|---------|----------|
| MAU[^1] | 2 млрд |  
| DAU[^3] (stickiness 30%) | 600 млн |  
| Среднее число запусков приложения в месяц[^3] | 50 |  
| Среднее число сессий в день (расчёт) | 1,67 | расчёт 
| Количество мест в базе[^1] | 250 млн |  

---

### Среднее количество действий одного пользователя в день

| Действие                        | Повторений в день | Обоснование |
|---------------------------------|-------------------|-------------|
| Отображение карты               | 20,0               | В среднем 50 запусков в месяц ≈ 1,67 в день |
| Отображение информации о местах | 3,3               | В среднем 2 запроса на сессию при 1,67 сессии в день |
| Поиск мест и организаций        | 2,4               | 71% сессий включают поиск[^4], по 2 запроса: 1,67 × 0,71 × 2 ≈ 2,4 |
| Построение маршрутов            | 1,3               | 75% сессий включают построение маршрута[^4]: 1,67 × 0,75 ≈ 1,3 |
| Следование по маршруту          | 8,0               | В среднем 1 маршрут в день. Средняя поездка 30 минут, обновление позиции каждые 15 секунд - 120 обновлений. Используется батчинг и агрегация информации о пробках и ETA. В результате из 120 обновлений реально превращается примерно в 8 сетевых запросов |
|Загрузка тайлов во время навигации (prefetch + viewport)|16,0|Prefetch ~150 тайлов/маршрут по 12 за запрос, viewport ~3 запроса по 24 тайла - суммарно ~16 сетевых запроса/сессию|
| Загрузка оффлайн-карт           | 0,02              | Загрузка/обновление ≈ 1 раз в 2 месяца |

1. Сырой объём точек: при 30-минутной поездке и интервале 15 с получается 120 точек. 
2. Обычно делают агрегацию/батчинг: они группируют несколько GPS-точек и метаданных в один сетевой вызов. Практические реализации группируют 10–20 точек в один пакет
3. Допустим, клиент аггрегирует в среднем по 15 точек в пакет → 120 / 15 = 8 пакетных отправок
---

### Технические метрики

В базе ~250 млн объектов[^1]. Основные поля и предполагаемый размер записи:

| Поле                     | Описание                      | Средний объем на 1 объект |
| ------------------------ | ----------------------------- | ------------------------- |
| **ID места**             | Уникальный идентификатор      | 16 байт                   |
| **Название**             | Текст (UTF-8)                 | 50 байт                   |
| **Категория**            | Тип места (кафе, отель, парк) | 16 байт                   |
| **Координаты (lat/lon)** | Два float-числа               | 16 байт                   |
| **Часы работы**          | JSON-объект                   | 200 байт                  |
| **Отзывы**               | Список текстовых данных       | ~1 КБ (по 10 отзывов)     |
| **Рейтинг**              | Число с плавающей точкой      | 8 байт                    |
| **Доп. данные**          | Телефон, сайт, соцсети        | 200 байт                  |

Согласно официальному Places API[^7], есть ещё много дополнительных полей (например, типы мест, метки, варианты парковки и тд), поэтому **округляю средний размер объекта до 3 КБ**.  

### Информация о местах: 250 млн × 3 КБ ≈ **720 ГБ**  
### Размер кэша тайлов: **1250 ГБ**
- Зумы 10–16: ≈ **5.7 млрд тайлов**
- Только 25–30% реально кэшируется (города, дороги) → **~1.71 млрд тайлов**
- Объём: 1.71e9 × 600 Б = **1026 ГБ**
- С запасом (разные форматы, метаданные) → **~1300 ГБ**

**Итоговое хранилище ≈ 2 ТБ.**

---

### RPS по типам запросов

**DAU = 600 млн.**  
**Среднее время суток = 86 400 секунд.**  
Пиковая нагрузка = 3× среднего значения.

Отображение карты: 20,0 (действий в сутки) / 86400 (секунд в сутках) * 600M DAU ≈ 13.9K

| Тип запроса                | Действий/день | Средний RPS     | Пиковый RPS (×3) |
|----------------------------|---------------|------------------|------------------|
| Отображение карты          | 20,0           | 138,9K            | **416,7K**        |
| Информация о местах        | 3,3           | 23,2K            | **69,6K**        |
| Поиск мест                 | 2,4           | 16,5K            | **49,5K**        |
| Построение маршрутов       | 1,3           | 9,0K             | **27,0K**        |
| Следование по маршруту     | 8,0           | 55,6K            | **166,7K**       |
|Загрузка тайлов во время навигации (prefetch + viewport)|16,0|111,1K|**333,3K**|
| Загрузка оффлайн-карт      | 0,02          | 139              | **417**          |
| Добавление отзывов     | 0,003         | 8               | **24**           |
| Редактирование карт    | 0,002         | 6               | **18**           |

---

### Сетевой трафик

| Тип запроса                | Размер 1 запроса | Средний RPS | Пиковый RPS | Пиковый трафик |
|-----------------------------|------------------|-------------|-------------|----------------|
| Отображение карты           | 7,2 КБ (~12 тайлов за раз при 1080p при 16 уровне масштабирования) | 138,9K | 416,7K | **~23,96 Гбит/с**  |
| Информация о местах         | 3 КБ             | 23,2K | 69,6K | **~1,67 Гбит/с**   |
| Поиск мест                  | 1 КБ (10 результатов)            | 16,5K | 49,5K   | **~0,4 Гбит/с**   |
| Построение маршрутов        | 5,2 КБ (Маршрут ~10 км: ~200 точек × 16 Б = 3,2 КБ + метаданные (~2 КБ) → ≈ 5,2 КБ)          | 9,0K  | 27,0K | **~1,12 Гбит/с** |
| Следование по маршруту        | 2,5 КБ (обновление позиции, ETA)| 55,6K      | 166,7K      | **~3,3 Гбит/с** |
|Загрузка тайлов во время навигации (prefetch + viewport)|21,6 КБ (600 B / тайл  (prefetch), 14,4 KB / viewport)|111,1K|333,3K|**~57,5Гбит/с**|
| Загрузка оффлайн-карт       | 10 МБ (≈ 17 000 тайлов уровня 16, один тайл ≈ 0,12 км², общая площадь ≈ 2 048 км²)            | 139   | 417   | **~33,4 Мбит/с**  |
| Добавление отзывов          | 2 КБ  | 8       | 24      | **~0,38 Мбит/с** |
| Редактирование карт         | 3 КБ  | 6       | 18      | **~0,43 Мбит/с** |

**Площадь одного тайла на уровне 16**  

- На уровне 16 весь земной шар делится на 2^16 * 2^16 = 65,536 * 65,536 тайлов.  
- Земная поверхность ≈ 510 млн км².  
- Площадь одного тайла:  ≈ 0,12 км² 

**Общая площадь оффлайн-карты**  

- Количество тайлов: 17 067  
- Общая площадь: 17 067 × 0,12 км² ≈ 2 048 км²
---

# 3. Глобальная балансировка нагрузки

## Обоснования расположения ДЦ (влияние на продуктовые метрики)

Распределение пользователей по странам (по данным hypestat[^10] и SimilarWeb App Intelligence[^2]) показывает, что наибольшая доля трафика приходится на США. Далее следуют Индия, Китай, Бразилия и Япония.  
Чтобы минимизировать сетевые задержки и обеспечить высокую доступность, дата-центры целесообразно размещать в ключевых точках присутствия пользователей.

**Выбранные регионы для ДЦ:**
- **США**: Нью-Йорк (Восток), Лос-Анджелес (Запад), Чикаго (Центр)  
- **Европа**: Франкфурт-на-Майне (низкие задержки для Европы и части Африки)  
- **Азия**: Мумбаи (Индия), Пекин (Китай), Токио (Япония)  
- **Южная Америка**: Сан-Паулу (Бразилия)  

Такое расположение обеспечивает покрытие всех ключевых регионов, снижает latency и повышает отказоустойчивость.

<img width="1549" height="698" alt="image" src="https://github.com/user-attachments/assets/7316186e-372e-490d-af73-6acc5f602dbd" />


---

## Распределение нагрузки по регионам

| Тип запроса                        | Нью-Йорк | Лос-Анджелес | Чикаго | Франкфурт | Мумбаи | Пекин | Сан-Паулу | Токио |
| ---------------------------------- | -------- | ------------ | ------ | --------- | ------ | ----- | --------- | ----- |
| **Отображение карты** (416.7K)     | 52.1K    | 52.1K        | 52.1K  | 62.5K     | 62.5K  | 62.5K | 62.5K     | 62.5K |
| **Информация о местах** (69.6K)    | 8.7K     | 8.7K         | 8.7K   | 10.9K     | 10.9K  | 10.9K | 10.9K     | 10.9K |
| **Поиск мест** (49.5K)             | 6.2K     | 6.2K         | 6.2K   | 7.8K      | 7.8K   | 7.8K  | 7.8K      | 7.8K  |
| **Построение маршрутов** (27.0K)   | 3.4K     | 3.4K         | 3.4K   | 3.9K      | 3.9K   | 3.9K  | 3.9K      | 3.9K  |
| **Следование по маршруту** (166.7K)| 20.8K    | 20.8K        | 20.8K  | 20.8K     | 27.8K  | 27.8K | 27.8K     | 27.8K |
| **Загрузка тайлов** (333.3K)       | 41.7K    | 41.7K        | 41.7K  | 41.7K     | 62.5K  | 62.5K | 21.4K     | 20.1K |
| **Оффлайн-карты** (417)            | 52       | 52           | 52     | 52        | 52     | 52    | 52        | 52    |

> Распределение сделано пропорционально MAU в регионах. США обслуживаются тремя узлами для равномерного снятия нагрузки.

---

## Выбор схемы глобальной балансировки

Оптимальной схемой является **Anycast-балансировка в сочетании с Geo-Based DNS**.

- **Geo-Based DNS**:
  - Используется для глобального распределения пользователей по регионам (США, Европа, Азия, Южная Америка).  
  - Позволяет направлять запросы в ближайший дата-центр на уровне DNS.  
  - Обеспечивает резерв в случае полной недоступности Anycast-кластера в регионе.  

- **Anycast-балансировка**:
  - Применяется внутри США, где расположены несколько дата-центров (Нью-Йорк, Лос-Анджелес, Чикаго).  
  - Один IP-адрес анонсируется в нескольких точках, маршрутизация BGP направляет трафик в ближайший узел.  
  - Снижает задержки за счёт выбора кратчайшего сетевого пути.  
  - Распределяет трафик между несколькими узлами и повышает устойчивость к DDoS-атакам.  
  - Позволяет быстро выводить узлы из работы при отказе (BGP withdraw) без ожидания обновления DNS-записей.  

### В итоге:

- **Основной механизм**: Geo-Based DNS для глобального распределения по регионам.  
- **Внутрирегиональный механизм**: Anycast в США для балансировки между несколькими дата-центрами.  
- **Fallback**: при отказе всего регионального Anycast-кластера DNS перенаправляет пользователей в соседний регион.  
- минимальные задержки на глобальном уровне.  
- отказоустойчивость при сбоях отдельных узлов или целых регионов.  
- масштабируемость и устойчивость к DDoS.  

## 4. Локальная балансировка нагрузки

### Балансировщик уровня L7
Выбран **NGINX**, так как он сочетает высокую производительность, гибкость конфигурации и широкую поддержку протоколов.  
Он позволяет:  
- выполнять **SSL termination на краю**, снижая нагрузку на backend-сервисы;  
- **кешировать тайлы и данные о местах**, уменьшая количество обращений к генераторам и базам данных;  
- **сжимать ответы** (gzip/Brotli), что особенно эффективно для текстовых данных (Place Details, результаты поиска).

### Алгоритмы балансировки
Используются разные политики в зависимости от типа трафика:  
- **Consistent Hash** — для тайлов и статических ресурсов (пик: **333,3K RPS** глобально). Обеспечивает, что один и тот же тайл всегда обрабатывается одним бэкендом, что **повышает cache-hit до 80–90%**.  
- **Least Connections** — для динамических запросов: поиск мест (**49,5K пик RPS** глобально) и построение маршрутов (**27K пик RPS**), так как время их обработки сильно варьируется.  
- **Round Robin** — применяется для вспомогательных или равномерных потоков с низкой нагрузкой.

### Отказоустойчивость
- **Kubernetes** автоматически перезапускает поды при сбоях и масштабирует их под нагрузкой.  
- **NGINX** выполняет активные health-check’ и пассивную проверку по живым запросам.  
- Балансировщики разворачиваются **минимум в 3 экземплярах** в разных зонах доступности (AZ), чтобы исключить **single point of failure**.

### Расчёт количества балансировщиков (на примере Франкфурта)
Согласно вашей таблице распределения нагрузки по регионам, пиковый RPS в **Франкфурте**:

| Тип запроса                | Пиковый RPS в Франкфурте |
|----------------------------|---------------------------|
| Отображение карты          | 62,5K                     |
| Информация о местах        | 10,9K                     |
| Поиск мест                 | 7,8K                      |
| Построение маршрутов       | 3,9K                      |
| Следование по маршруту     | 20,8K                     |
| Загрузка тайлов            | 41,7K                     |
| Оффлайн-карты              | 52                        |
| **Итого**                  | **≈ 147 600 RPS**         |

**Допущение:**  
Один NGINX-инстанс в продакшене с SSL termination и смешанной нагрузкой (статика + динамика) способен обрабатывать **до 10 000 RPS** при сохранении 99-го процентиля latency **< 100 мс**. Это соответствует типичной производительности облачного инстанса среднего класса (2 vCPU, 8 ГБ RAM) и подтверждается бенчмарками NGINX Inc.

Применяем формулу резервирования:

$$
N_\text{req} = \left\lceil \frac{RPS_\text{пик} \times S}{RPS_\text{балансировщика} \times U} \right\rceil
$$

где:  
- $RPS_\text{пик} = 147\,600$,  
- $S = 1.15$ — запас на кратковременные всплески,  
- $RPS_\text{балансировщика} = 10\,000$,  
- $U = 0.7$ — целевой уровень загрузки.

$$
N_\text{req} = \left\lceil \frac{147\,600 \times 1.15}{10\,000 \times 0.7} \right\rceil = \left\lceil \frac{169\,740}{7\,000} \right\rceil = \left\lceil 24.25 \right\rceil = 25
$$

С учётом отказоустойчивости и распределения по **3 зонам доступности**, разворачиваем **27 инстансов NGINX** (9 в каждой AZ).

Аналогичный расчёт может быть выполнен для любого другого региона на основе вашей таблицы распределения нагрузки.

### Итог
NGINX выбран как оптимальное L7-решение: он поддерживает разные стратегии балансировки, интегрируется с Kubernetes и обеспечивает обработку пиковой нагрузки (до **147K RPS в Франкфурте**) при сохранении низких задержек и высокой отказоустойчивости.
---

## Балансер балансеров

Для управления множеством NGINX-балансировщиков в каждом регионе вводится **L4-балансер балансеров**, размещённый в каждом ДЦ.

### Архитектура:
- На глобальном уровне: **GeoDNS - ближайший регион**.
- Внутри региона: **L4-балансер (например, Google Cloud Load Balancer)** распределяет трафик между **группами NGINX-инстансов**.
- Этот L4-балансер работает на транспортном уровне (TCP/UDP), обеспечивая:
  - Высокую пропускную способность,
  - Минимальные задержки,
  - Health-check всех NGINX-нод.

> Запросы на запись направляются **только в выделенную группу NGINX для записи**, которая перенаправляет их на **мастер-сервер**.

---

## Отдельный домен для тайлов

Для оптимизации доставки статики и снижения нагрузки на основной API вводится **отдельный поддомен**:

- **`tiles.google.com`** - используется **только для загрузки тайлов**.
- Этот поддомен:
  - Обслуживается **отдельной группой NGINX**, настроенных на **Consistent Hash** по пути тайла (`/z/x/y.png`);
  - Использует **CDN** с TTL = 30 дней;
  - Имеет **собственный пул серверов хранения тайлов**.

---

## Отдельные входные группы серверов

В архитектуре выделяются **две независимые группы серверов**:

| Группа | Назначение | Тип запросов | Серверы |
|--------|------------|--------------|--------|
| **Read-only (RO)** | Обслуживание чтения | Отображение карт, поиск, маршруты, информация о местах, тайлы | Множество реплик баз данных, тайловые серверы |
| **Read-write (RW)** | Обработка записи | Добавление отзывов, редактирование карт | Мастер-ноды БД, сервисы версионирования |

### Мастер-сервер записи: единый источник
Все операции записи обрабатываются **только мастер-сервером**, размещённым в **Нью-Йорке**. Он:
- Принимает и валидирует запросы `POST /places/{place_id}/reviews` и `POST /maps/versions`;
- Генерирует новые версии карт
- Записывает изменения
- Публикует события в очередь для репликации.

#### Почему только в Нью-Йорке?
1. **Консистентность**: избегаем конфликтов версий при глобальной записи.
2. **Модерация**: единый пайплайн проверки (спам, геоошибки).
3. **Низкая нагрузка**: пик - всего **~4 RPS**.
4. **Юридическая простота**: единая юрисдикция для аудита.

#### Сколько серверов записи?
- Пиковая нагрузка: **4 RPS**;
- Производительность одного сервера: **100 RPS**;
- С учётом отказоустойчивости: **3 сервера** в трёх AZ Нью-Йорка (один активный — мастер, два в standby).

> Все запросы на запись направляются **только в эту группу**. Изменения реплицируются в RO-регионы асинхронно.
<img width="3253" height="1392" alt="image" src="https://github.com/user-attachments/assets/adc45a49-0fca-458e-a1a2-b748c61ad8ca" />




# 5. Логическая схема БД

<img width="1352" height="911" alt="Untitled" src="https://github.com/user-attachments/assets/d66dcb80-9ecb-49e9-af87-72bbe202e59c" />





## Размеры таблиц

| **Таблица**         | **Описание**                                                                                     | **Размер записи (байт)** | **Число записей**                  | **Объем** |
| ------------------- | ------------------------------------------------------------------------------------------------ | ------------------------ | ---------------------------------- | --------- |
| **category**        | Категории мест (рестораны, магазины, парки и т.д.)<br>Поля: `id`, `title`, `created_at`          | 24                       | ~100                               | 2.4 KB    |
| **user**            | Пользователи: `id`, `name`, `email`, `password_hash`, `is_local_guide`, `created_at`, `updated_at` | 32                       | 2B                                 | 59.6 GB   |
| **place**           | Места: `id`, `title`, `desc`, `category_id`, `member_id`, `member_type`, `rating`, `phone`, `created_at`, `updated_at` | 663                      | 250M                               | 154 GB    |
| **review**          | Отзывы: `place_id`, `user_id`, `text`, `rating`, `created_at`, `updated_at`                      | 152                      | 1.25B (по 5 отзывов на место)      | 177 GB    |
| **node**            | Географические точки: `id`, `coord`, `created_at`, `updated_at`                                  | 48                       | 9B                                 | 402 GB    |
| **node_tag**        | Атрибуты узлов: `id`, `node_id`, `key`, `value`, `created_at`                                    | 232                      | 906M                               | 196 GB    |
| **way**             | Пути (линии): `id`, `created_at`, `updated_at`                                                  | 16                       | 1B                                 | 15 GB     |
| **way_node**        | Связи "путь–узел": `id`, `way_id`, `node_id`, `sequence_id`                                     | 50                       | 5B                                 | 233 GB    |
| **way_tag**         | Атрибуты путей: `id`, `way_id`, `key`, `value`                                                  | 232                      | 2.5M                               | 553 MB    |
| **relation**        | Сложные связи: `id`, `created_at`, `updated_at`                                                 | 16                       | 13M                                | 198 MB    |
| **relation_tag**    | Атрибуты связей: `id`, `relation_id`, `key`, `value`, `created_at`                               | 232                      | 53M                                | 11.5 GB   |
| **relation_member** | Участники связей: `id`, `relation_id`, `member_id`, `member_type`, `sequence_id`, `created_at`   | 54                       | 91M                                | 4.5 GB    |
| **member_type**     | Тип участника (`node`, `way`, `relation`)                                                       | 102                      | 3                                  | 306 B     |
| **tile_cache**      | Кэш тайлов: `id`, `x`, `y`, `z`, `file_path`, `rendered_at`, `expires_at`                        | 44                       | 1.6B                               | 65.6 GB   |
| **map_version**     | Версии карт: `id`, `created_at`, `author_id`, `changeset`, `version_comment`                     | 80                       | ~10K                               | < 1 MB    |
| **ИТОГО:**          |                                                                                                  | —                        | —                                  | **≈1.3 TB** |

## Нагрузки (QPS) на чтение/запись

### Кэш

| Zoom  | Что видно             | Типичный юзкейс                                        | Поведение                 | Доля запросов | Взятое значение |
| ----- | --------------------- | ------------------------------------------------------ | ------------------------- | ------------- | --------------- |
| 0–5   | Весь мир / континенты | Почти неиспользуемы. Только при первом открытии карты. | Очень редкие              | ~1%           | 1%              |
| 6–8   | Страны / регионы      | Люди рассматривают страны, маршруты между городами     | Немного больше            | ~4%           | 4%              |
| 9–12  | Города, районы        | **Основной уровень навигации, просмотра, поиска**      | Основа кэша               | **65–75%**    | 70%             |
| 13–16 | Кварталы, улицы       | Подробный просмотр улиц, работа с POI                  | Очень часто при навигации | ~20–25%       | 23%             |
| 17–20 | Дома, здания          | Почти только при встраивании, просмотре входов/точек   | Узкие сценарии            | ~2–4%         | 2%              |


Для снижения нагрузки на базу данных в картографическом сервисе применяется кэширование тайлов. Будем применять lazy-render с кэшированием тайлов. Сразу после рендера, тайл кэшируется. Тайлы сохраняются на диске в виде файлов `/tiles/{z}/{x}/{y}.png`, а информация о них хранится в служебной таблице `tile_cache`, где фиксируются дата рендера (`rendered_at`), срок актуальности (`expires_at`).

#### Логика кэширования:

- Если тайл актуален — он отдается из кэша.

- Если тайл отсутствует или устарел — он рендерится, сохраняется и кешируется.

#### Инвалидация кэша:

При изменении геоданных вычисляется `bounding box` изменения, и для всех тайлов, которые пересекаются с этой областью, в `tile_cache` проставляется метка об устаревании. При следующем запросе эти тайлы будут перерендерены.

#### Консистентность тайлов:

Чтобы избежать ситуации, когда один тайл отображает новые данные, а соседний — старые, при каждом запросе дополнительно проверяются восемь соседних тайлов. Если хотя бы один из них был перерендерен на более новой версии данных, текущий тайл также считается устаревшим и рендерится заново.

Такой подход позволяет:

- избежать несостыковок между тайлами,

- отказаться от полной предгенерации,

- и снизить нагрузку на базу данных при массовых чтениях.


| **Таблица**  | **Описание**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | **Индексы**                                                                                                                                                                                                                                                                                                                                                                                      | **Read QPS** | **Write QPS** |
| ------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------- | ------------- |
| **user**     | Минимальные данные об авторах отзывов. Полные профили хранятся в Google Accounts. <br/>**R:** При получении **информации о местах** (69.6K RPS) подгружаются отзывы и данные пользователей: `69.6K × 5 = 348K QPS`.<br/>**W:** Добавление записей только при создании новых отзывов (`review`).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | `CREATE INDEX user_id_idx ON user (id);`<br/>`CREATE INDEX user_name_trgm_idx ON user USING GIN (name gin_trgm_ops);`                                                                                                                                                                                                                                    | 348K          | —             |
| **place**    | Хранит основные сведения о местах (POI). <br/>**R:** Информация о местах + Поиск мест = `69.6K + 49.5K = 119.1K`.<br/>**W:** Пользовательские правки редки, глобально ~2 RPS.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `CREATE INDEX place_id_idx ON place (id);`<br/>`CREATE INDEX place_title_trgm_idx ON place USING GIN (title gin_trgm_ops);`<br/>`CREATE INDEX place_category_idx ON place USING BTREE (category);`<br/>`CREATE INDEX place_coord_idx ON place USING GIST (coord);`                                                                                       | 119.1K        | 2             |
| **category** | Категории мест. Изменяются редко, читаются при запросах информации и поиска.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | `CREATE INDEX category_id_idx ON category (id);`<br/>`CREATE INDEX category_name_idx ON category (name);`                                                                                                                                                                                                                                                 | 119.1K        | 0             |
| **review**   | Отзывы пользователей. <br/>**R:** Информация о местах `69.6K × 5 отзывов = 348K QPS`.<br/>**W:** ~2 млн отзывов в день → **~23 RPS**.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | `CREATE INDEX review_place_user_idx ON review (place_id, user_id);`<br/>`CREATE INDEX review_date_idx ON review (created_at DESC);`<br/>`CREATE INDEX review_rating_idx ON review (rating);`                                                                                                                                                               | 348K          | 23            |
| **map_version** | Метаданные версий карт при редактировании. Создаётся при каждой правке.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | `CREATE INDEX map_version_id_idx ON map_version (id);`<br/>`CREATE INDEX map_version_date_idx ON map_version (created_at DESC);`                                                                                                                                                                                                                           | —             | 2             |

> **Важно**: Все операции записи (`review`, `place`, `map_version`) направляются **только в мастер-БД в Нью-Йорке**. RO-регионы получают данные через **асинхронную репликацию**. Поэтому Write QPS в таблицах выше относится **исключительно к мастер-ноде**.

### Таблицы с геоданными

Таблицы `node`, `node_tag`, `way`, `way_node`, `way_tag`, `relation`, `relation_tag`, `relation_member`, `member_type` используются при отображении карты, поиске и маршрутизации.

| **Таблица**               | **Описание** | **Индексы** | **Read QPS** | **Write QPS** |
| -------------------------- | ------------- | ------------- | ------------- | ------------- |
| **node**                   | Точки (координаты) объектов карты.                                                                                                                                               | `CREATE INDEX node_id_idx ON node (id);`<br/>`CREATE INDEX node_coord_gist_idx ON node USING GIST (coord);`                                                                                                                                                              | (`69.6K`) Информация о местах + <br>(`49.5K`) Поиск мест + <br>(`27K`) Маршруты + <br>(`166.7K`) Навигация = **~312.8K QPS** | **2** |
| **node_tag**               | Теги для `node`.                                                                                                                                                                  | `CREATE INDEX node_tag_node_idx ON node_tag (node_id);`<br/>`CREATE INDEX node_tag_key_idx ON node_tag (key);`                                                                                                                                                            |                   |               |
| **way**                    | Линейные объекты (улицы, дороги).                                                                                                                                                 | `CREATE INDEX way_id_idx ON way (id);`                                                                                                                                                                                                                                    |                   |               |
| **way_node**               | Связи между `way` и `node`.                                                                                                                                                        | `CREATE INDEX way_node_way_idx ON way_node (way_id);`<br/>`CREATE INDEX way_node_node_id_idx ON way_node (node_id);`                                                                                                                                                      |                   |               |
| **way_tag**                | Теги для `way`.                                                                                                                                                                   | `CREATE INDEX way_tag_key_idx ON way_tag (key);`                                                                                                                                                                                                                          |                   |               |
| **relation**               | Сложные объекты (например, маршруты, административные границы).                                                                                                                    | `CREATE INDEX relation_id_idx ON relation (id);`                                                                                                                                                                                                                          |                   |               |
| **relation_member**        | Связи между `relation` и членами (`node`, `way`).                                                                                                                                 | `CREATE INDEX relation_member_member_id_idx ON relation_member (member_id);`                                                                                                                                                                                              |                   |               |
| **relation_tag**           | Теги для `relation`.                                                                                                                                                              | `CREATE INDEX relation_tag_key_idx ON relation_tag (key);`                                                                                                                                                                                                                |                   |               |
| **member_type**            | Типы членов в `relation` (node/way/relation).                                                                                                                                      | `CREATE INDEX member_type_id_idx ON member_type (id);`                                                                                                                                                                                                                    |                   |               |


# 6. Физическая схема БД
<img width="840" height="618" alt="image" src="https://github.com/user-attachments/assets/67017881-73fb-40ff-97b5-31132cdc5f1a" />


## Выбор СУБД и хранение тайлов

| **Тип данных**    | **СУБД / Хранение**                   | **Причины выбора / Комментарий**                                                                                  |
| ----------------- | ------------------------------------ | ----------------------------------------------------------------------------------------------------------------- |
| Геоданные         | PostgreSQL + PostGIS + pgRouting     | Пространственные данные, индексы по координатам, маршрутизация.                                                  |
| Метаданные        | Apache Cassandra                     | Горизонтальная масштабируемость, оптимизация под большое число операций записи, эффективное хранение информации о местах и отзывах. |
| Тайлы карты       | GCS (.webp, .pbf)                    | Объём ~1.7 млрд тайлов × 600 Б ≈ 1 ТБ. Хранение бинарных файлов вне базы.                                        |

---


## Индексы

- **Пространственные запросы**:
  - Для ускорения работы с пространственными запросами создадим пространственные индексы для таблицы `place_on_map`: `CREATE INDEX place_on_map_coord_idx ON place_on_map USING GIST (coord);`
  - Так как `node` огромное количество (9B записей), и требуется искать все `node`, которые попадают в viewport для рендеринга, то обязательно нужен индекс для поиска по координатам `node`: `CREATE INDEX node_coord_gist_idx ON node USING GIST (coord)`
- **От `node` к сложным объектам (`way` & `relation`)**
  - Для быстрого поиска `way_node` (5B записей) по `node_id` создаем индекс: `CREATE INDEX way_node_node_id_idx ON way_node (node_id);
  - Аналогично для быстрого поиска `way` из `way_node` и `relation_member` создаем индекс по `PK`: `CREATE INDEX way_id_idx ON way (id)`
  - На `node` и `way` ссылается `relation_member.member_id`, поэтому нужно создать индекс по этому полю для эффективного поиска `relation` и `relation_member` по `node_id` и `way_id`: `CREATE INDEX relation_member_member_id_idx ON relation_member (member_id);`
- **Для ускорения текстового поиска**:
  - по названию **GIN индекс** для поиска (даже с опечатками): `CREATE INDEX place_on_map_title_trgm_idx ON place_on_map USING GIN (title gin_trgm_ops)
  - по категории места B-tree индекс для **точного** совпадения: `CREATE INDEX place_on_map_category_idx ON place_on_map USING BTREE (category)`
- **От сложных объектов (`way` & `relation`) к простым (`node`)**
  - Поиск всех `node` внутри `way`: Индекс на `way_node.way_id`: `CREATE INDEX way_node_way_idx ON way_node (way_id)
  - Быстрый поиск `way` по PK: уже созданных индекс `way_id_idx`
  - Быстрый поиск `node` по PK: `CREATE INDEX node_id_idx ON node (id)`
- **Теги**:
  - Быстрый поиск тегов для точки (наибольший объем по тегам у таблицы `node` в сравнении с `way` & `relation`): индекс на `node_tag.node_id`: `CREATE INDEX node_tag_node_idx ON node_tag (node_id)`
  - Поиск тегов по ключу (B-Tree):
    - `CREATE INDEX node_tag_key_idx ON node_tag (key)`
    - `CREATE INDEX way_tag_key_idx ON way_tag (key)`
    - `CREATE INDEX relation_tag_key_idx ON relation_tag (key)`

## Репликация и шардирование

- Для таблиц `node`, `node_tag`, `way`, `way_node`, `relation`, `relation_tag`, `relation_member`, `member_type` будем использовать 16 реплик на чтение. (312 800 QPS ÷ 16 реплик ≈ 19 550 QPS на реплику) Воспользуемся каскадной репликацией, чтобы избежать нагрузки на master.
- Для таблицы `review` делаем композитный ключ, в котором `place_id` - это partition_key, а `user_id` - это clustering_key.

### Партиционирование

- Структура таблиц `way_tag`, `relation_tag`, `node_tag` идентична. Это 1 таблица, разделенная на три для ускорения операций получения тегов. Таким образом при получении тегов `way`, не нужно проходиться по тегам `relation` & `node`.


## Клиентские библиотеки / интеграции

Бэкэнд будет написан на Go из-за его высокой производительности и популярности.

- PostgreSQL - [pgx](https://github.com/jackc/pgx)
- Apache Cassandra - [Apache Cassandra GoCQL Driver](https://github.com/apache/cassandra-gocql-driver)

- PostgreSQL: PgBouncer (мультиплексирование подключений)
- Apache Cassandra: балансировка на уровне драйвера

## Схема резервного копирования

- PostgreSQL: каждые сутки `pg_basebackup`, каждые 5 минут `WAL-G`
- Apache Cassandra: каждые сутки `nodetool snapshot`, каждые 5 минут `Incremental Commit Logs`

# 7. Алгоритмы

**MetaTile** - техника оптимизации рендеринга тайлов карты.  
Вместо рендеринга каждого отдельного тайла (256×256 px) сервер рендерит **большую область** (например, 4×4 тайла = 1024×1024 px), а затем **разрезает** её на 16 стандартных тайлов.

### Проблема
Рендеринг каждого тайла по отдельности создаёт:
- миллионы мелких запросов к рендереру;
- нагрузку на CPU и диск;
- несостыковки на границах (расхождение линий, подписей и текстур).

### Решение
1. При запросе тайла сервер вычисляет **ключ метатайла** (по координатам x, y, z).  
2. Проверяется наличие готового метатайла в **tile_cache** (PostgreSQL).  
3. Если нет — выполняется рендеринг всей области 4×4 тайлов (1024×1024 px) через **Mapnik**.  
4. Результат разрезается на 16 отдельных тайлов (256×256 px).  
5. Тайлы в формате `.webp` и `.pbf` сохраняются во **внешнем хранилище GCS**.  
6. В PostgreSQL записывается метаинформация (`tile_cache`): путь, хэш, TTL, версия, `rendered_at`.  
7. В Cassandra сохраняются данные о слоях, атрибутах и связях тайлов с геообъектами.

### Эффект
- Сокращается количество операций рендеринга в 16 раз.  
- Исключаются визуальные артефакты на границах.  
- Увеличивается cache-hit на уровне CDN и tile-серверов.  
- Снижается нагрузка на PostgreSQL и рендер-очередь.

---

## Алгоритм инвалидации MetaTile

1. При обновлении геоданных (новый объект, изменение геометрии, удаление) вычисляется **bounding box**.  
2. Определяются все метатайлы, пересекающие этот bounding box.  
3. Эти метатайлы помечаются как **устаревшие** (`is_valid = false` в `tile_cache`).  
4. В очередь (Kafka / PubSub) добавляется задача на **перерендеринг**.  
5. После рендеринга обновлённые тайлы записываются в GCS, а записи в PostgreSQL и Cassandra обновляются.

---

## Поток обработки данных

1. Пользователь запрашивает тайл `/z/x/y.pbf`.  
2. Tile API проверяет кэш в PostgreSQL.  
3. Если тайл отсутствует или устарел — рендерится **MetaTile (4×4)**.  
4. Разрезается и сохраняется в GCS.  
5. Метаданные синхронизируются между PostgreSQL и Cassandra.  
6. Клиент получает готовый тайл через CDN.

---
## Источники
[^1]: [Почему Google?](https://mapsplatform.google.com/why-google/)  
[^2]: [Анализ веб-трафика Google Maps](https://pro.similarweb.com/#/digitalsuite/websiteanalysis/audience-geography/*/999/3m?key=maps.google.com&webSource=Total)  
[^3]: [Статистика и тренды Google Maps](https://www.loopexdigital.com/blog/google-maps-statistics)  
[^4]: [Как часто россияне используют геосервисы](https://iom.anketolog.ru/2020/02/12/geoservisy)  
[^5]: [Размер тайлов и кэша OSM](https://switch2osm.org/serving-tiles/plan/)  
[^6]: [Google Maps Statistics 2024](https://www.onthemap.com/blog/google-maps-statistics/)  
[^7]: [API](https://developers.google.com/maps/documentation/places/web-service/data-fields)
[^10]: [Hypestat – аудитория и география трафика](https://hypestat.com/info/maps.google.com) 
